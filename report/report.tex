\documentclass[pageno]{jpaper}
\newcommand{\IWreport}{2012}
\usepackage[normalem]{ulem}
\begin{document}

\title{Holograsp: Bringing Virtual Objects into the Real World for Natural 3D Manipulation}

\date{}
\author{Edward Zhang}
\maketitle

\thispagestyle{empty}

\begin{abstract}
Traditional computer interfaces, namely the mouse and monitor, are inherently 2D. Thus, performing 3D tasks using
these interfaces is often challenging because both input and output are missing a dimension. Surprisingly,
previous work has consistently found the mouse to be superior to custom devices with three or more degrees
of freedom (DOF). We hypothesize that the disconnect between the input device and the visual feedback is the
source of the poor performance of high DOF devices. To investigate this hypothesis, we developed the Holograsp
system. Holograsp uses a stereoscopic 3D monitor to display virtual 3D objects as if they were floating in
space in front of the monitor; users can reach out and grasp these models to move them, as if they were real
objects. We performed an experiment comparing the efficacy of Holograsp with traditional 2D input and output
interfaces by measuring the time taken to complete 3D placement tasks. The results show that, although the
pop-out stereoscopic effects were difficult for some people to perceive, the combination of stereoscopic 3D
feedback and natural gestural interface was by far the most effective and intuitive system.
\end{abstract}

\section{Introduction}
% FIXME
In this paper we make two main contributions:
\begin{itemize}
\item We present Holograsp, a 3D manipulation system, to show the feasibility of colocating display and gestural spaces.
\item We show the benefits of such systems through a user study involving 3D manipulation tasks, in which we compare the
performance of Holograsp with traditional interfaces.
\end{itemize}

% ------------------------------------- RELATED WORK ----------------------------------------
\section{Related Work}
% FIXME
\subsection{Virtual and Augmented Reality}
\subsection{3D User Interfaces}
% In air interfaces, selection/manipulation, stereoscopy, reach in systems, virtual reality, augmented reality
% ------------------------------------- OVERVIEW --------------------------------------------
\section{Holograsp Overview}
% FIXME
% ------------------------------------- IMPLEMENTATION --------------------------------------
\section{Holograsp Implementation}
\subsection{Software Architecture}
Holograsp uses a heavily abstracted engine to facilitate substitution of input and output devices, client applications, and SDKs.
The architecture of the engine invites the construction of many client applications; Holograsp provides the alignment of the
gestural and display spaces while the client program is free to use the display and input in any way. Our example application
is the timed 3D manipulation task harness used in the experiments section. The incarnation of the architecture used in the
experiments used DirectX for graphics rendering, the Nvidia API for configuring the 3D monitor, and the Intel Perceptual
Computing SDK for input.
\subsubsection{Holograsp Engine}\\
The core engine uses the SDL library \footnote{http://www.libsdl.org/} for fundamental windowing and event handling. 
The engine performs the following tasks:
\begin{itemize}
\item Check for and handle mouse, keyboard, and application events.
\item Check for new data from gestural input devices.
\item Buffer and preprocess any new data into a standard form, specifically finger coordinates and depth maps.
\item Transform the raw data into application-specific data, such as parsing finger coordinates into a hand pose.
\item Call application specific update functions, such as translating and rotating objects or creating new models.
\item Render the scene in stereoscopic 3D.
\end{itemize}

Most of these tasks are delegated to their own handlers, which are often application-specific.

\subsection{Stereoscopic Display Hardware}
One of the core aspects of Holograsp is the stereoscopic 3D. There are two competing standards for stereoscopic 3D rendering for computers
that are tied to the main graphics card manufacturers, Nvidia and AMD. Nvidia's 3D Vision system \footnote{http://www.nvidia.com/object/3d-vision-main.html}
produces packaged hardware systems including high quality shutter glasses and wireless synchronization stations that are guaranteed to work with Nvidia
graphics cards and supported monitors; the AMD HD3D system does not provide a unified system \footnote{http://www.amd.com/us/products/technologies/amd-hd3d/Pages/hd3d.aspx}.

In our setup, we used an Nvidia GTX555 consumer graphics card with a 27 inch Acer VG278H 3D monitor. The standard Nvidia shutter glasses use infrared
signals to synchronize the shutters with the display; since all of the depth cameras we tested used infrared for depth information, this resulted in
unacceptable interference. To eliminate this interference, we used the shutter glasses and synchronization station from the Nvidia 3D Vision Pro
package, which uses radio frequencies for synchronization.

\subsection{Stereoscopic Rendering}
Nvidia's 3D Vision supports two methods of rendering in stereoscopic 3D. Quad-buffered stereo rendering allows developers to render the scene for each
eye separately in OpenGL, and provides maximal control of stereo effects. 3D Vision Automatic allows any application running using Direct3D to be
displayed in stereoscopic 3D, without any additional development overhead \cite{nvidia3dvision}. Quad-buffered stereo is only supported on higher-end
professional graphics cards, whereas 3D Vision Automatic is supported on all newer Nvidia graphics cards. Therefore, we elected to use 3D Vision Automatic
for stereoscopic display.

In 3D Vision Automatic, rendering in stereoscopic 3D is based on the use of homogeneous coordinates. 3D positions and transformations are represented
as 4D vectors and matrices, respectively, to allow for translation transformations \cite{graphicstextbook}. 3D Vision Automatic takes advantage of the
fact that, in homogenous coordinates, the 3D location of $(x, y, z, w)$ is equivalent to that of $(ax, ay, az, aw)$ for $a \ne 0$. The $w$-coordinate
of the coordinate vector is used in the standard projection matrix to store the eye-space depth; in normal rendering the developer generally assumes
$w=1$ and ignores the $w$ coordinate. In 3D Vision Automatic the $w$ coordinate is used to control the degree of stereoscopic effect, and the
developer can artificially set $w$ if appropriate \cite{nvidia3dvision}.

The stereoscopic effects in 3D Vision Automatic are controlled via abstracted options, primarily the eye separation and convergence depth.
3D Vision Automatic renders stereoscopically by duplicating any Direct3D calls that render to the backbuffer, and modifying the appropriate
transformation matrices so that the duplicated rendering instructions appear to have been issued for two eyes. The $x$-coordinate of any geometric
coordinates $(x_{2d}, y, z, w)$ is shifted into two stereo coordinates $(x_{left}, y, z, w)(x_{right}, y, z, w)$ based on the equations
$$x_{left} = x_{2d} + s(w - c)$$
$$x_{right} = x_{2d} - s(w - c)$$
where $s$ is the eye separation and $c$ is the convergence depth \cite{nvidia3dvision}. We note in particular that $w<c$ gives out of screen effects, where
the left eye image is to the right of the right eye image and vice versa. Unfortunately, these two quantities $s, c$ are in eye coordinates and not
physical coordinates, which necessitates a further remapping step for proper calibration.

Because of the constraints on stereoscopic rendering methods, Holograsp supports both OpenGL and Direct3D rendering APIs under a unified interface.
Currently, only simpler calls such as camera manipulation, transformation matrices, point rendering, and polygon rendering are supported. In addition,
we include fairly simple mesh rendering functionality. This general rendering interface is exposed to client applications. It is somewhat surprising that
a unified wrapper does not already exist, given the popularity of OpenGL and Direct3D; this component of Holograsp is one that can be easily extended for
more advanced rendering capabilities.

\subsection{Gestural Interface}
Our input interface is centered around the Creative Interactive Gesture Camera and the Intel Perceptual Computing SDK 
\footnote{http://software.intel.com/en-us/vcsource/tools/perceptual-computing-sdk}.
\subsubsection{Gestural Input Hardware and SDKs} \\
The CIGcam includes a microphone, a standard RGB camera, and a depth camera. Holograsp made heavy use of the depth camera functionality. 
The depth camera uses infrared time-of-flight methods to construct a depth image of resolution $320 \times 240$ pixels at a peak frame
rate of 30 frames per second. The RGB camera was used to construct point-cloud visualizations but was not used for gesture recognition;
the RGB camera provides resolutions of up to $1280 \times 720$ pixels at 30 frames per second. The microphone was not used in Holograsp.

The Intel Perceptual Computing SDK provided two capabilities that were heavily used in Holograsp. The more fundamental capability was
to expose the depth and camera streams so that Holograsp could extract and process individual data frames. The SDK also provided
hand and finger pose recognition, providing the client application with precise locations of hands, palms, and fingers. 

\subsubsection{Alternate Gestural Input Devices}\\
Many developments in 3D sensing hardware have been announced recently, starting with Microsoft's Kinect sensor. These sensing
devices are geared towards consumers and are inexpensive compared to earlier depth cameras, which were only used for specialized applications.
The CIGcam and its SDK were publicly released in October 2012; before acquiring a CIGcam, Holograsp used the Microsoft Kinect for Windows SDK 
\footnote{http://www.microsoft.com/en-us/kinectforwindows/} for input. In contrast to the CIGcam, the Kinect uses structured light
to obtain its depth images. This resulted in a different set of benefits to using either device.

\begin{tabular}{|c|c|}
\hline
\bf{Microsoft Kinect} & \bf{CIGcam} \\ \hline
Interaction range of 40cm to 400cm & Interaction range of 15cm to 100cm \\
$640 \times 480$ depth image resolution & $320 \times 240$ depth image resolution \\
Holes in the depth image at points of high curvature & No holes \\
Moderate depth noise, typically under 3cm in magnitude & Frequent depth noise, typically under 5cm in magnitude \\
SDK focused on whole-body skeletal tracking & SDK focused on finger and face tracking \\
\$150USD in cost & \$150 USD in cost \\
\hline
\end{tabular}

Ultimately, we selected the CIGcam for its more appropriate range of interaction and the finger-recognition
capabilities of the Intel SDK.

The Leap sensor by LeapMotion \footnote{https://leapmotion.com/} is a promising sensor that seems to provide extremely stable and
accurate hand tracking. Unfortunately, although we are members of the Leap early developer program, the device was not available
before user studies could take place. Future iterations of Holograsp will likely incorporate the Leap sensor and SDK.

\subsection{Gestural Input Processing}
There are two components to handling gestural input. The Layer-1 input processor extracts raw data from input devices, usually through specific
SDKs, and translates it into a standardized format for Holograsp applications. The second layer takes this data and performs heavier
parsing to extract higher level information from it. This includes translating finger coordinates into a hand pose, or extracting
points of interest from a depth image. This two layer system facilitates the substitution of alternate input devices separately from
gesture recognition algorithms.

Input device SDKs usually consists of a depth image at minimum. A depth image is a grid of pixels, where each 
pixel contains the distance from the camera to the nearest object along the ray through that pixel. Many SDKs
also provide a color image from a camera, as well as positions of hands, fingers, heads, or other body parts.
Layer-1 input processing classes tended to be simple wrappers for calls to the appropriate SDK, using a unified
interface.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/fsm.png}
\caption{Finite State Machine for Hand Selection States}
\label{fig:fsm}
\end{figure}
Holograsp currently has two Layer-2 input processing classes. The first examines finger coordinates to distinguish poses based
on the index finger and the thumb; the point of interest (used for a cursor or a selection point) would be the midpoint of the tip
of the thumb and the tip of the index finger, and different states were triggered based on whether the two fingers were pinched
together or were separate. This was accomplished via a simple finite state machine illustrated in Figure~\ref{fig:fsm}. We also
applied hysteresis to the state transitions to account for tracking noise; 5 consecutive frames indicating the same state transition
were necessary before a state transition occured. 

The second Layer-2 class has a simpler and tends to be more robust. The mechanism simply selects the closest blob of pixels to the monitor
as the point of interest. For speed and simplicity, the default algorithm looks at neighborhoods of nine pixels to determine the closest blob of pixels,
and takes the average of the corresponding points in space to be the point of interest. This mechanism also applies some simple filtering to 
smooth each coordinate of the resulting point, since this input method was subject to varying amounts of jitter. Several simple 
smoothing methods can be used; we found a double moving average filter with a window size of 5 to be appropriate for the data rates of
our input devices. Many filters are included in Holograsp's implementation, mostly adapted from \cite{brown2004smoothing}. This class was
used in the experiment application because of its robustness.

\subsection{Calibration of Coordinate Systems}
As Holograsp involves a spatially accurate input and output systems, there are two distinct calibration steps needed. The calibration
of camera coordinates with world coordinates is a well-understood problem in computer vision and augmented reality. 
However, it is much harder to calibrate the stereoscopic display space given the limitations of the 3D Vision Automatic
system.
\subsubsection{Camera to World Calibration}\\
% FIXME
\subsubsection{Stereoscopic Display to World Calibration}\\
The major benefit of using a stereoscopic 3D monitor instead of a head-mounted display is that the 3D calibration is less sensitive to
head movements when using the monitor.
% FIXME

Despite the fact that Holograsp does not perform viewpoint tracking, it is still possible to obtain
fairly consistent calibration results in a small range of head positions.
% FIXME
% ------------------------------------- METHDOLOGY ------------------------------------------
\section{Evaluation Methodology}
Many previous works, such as \cite{study1,study2,mattheiss2011navigating}, have consistently found the
mouse to be faster and more precise for 3D manipulation
tasks than devices with higher degrees of freedom, including in-air devices. 
The goal of this experiment was to determine if the poor performance of high DOF devices could
be a result of cognitive separation between the interface and the task to be performed.
\subsection{3D Manipulation Task}
Bowman et. al describe several fundamental tasks in 3D interfaces, primarily selection,
manipulation, navigation, system control, and symbolic input \cite{3dui}. The most fundamental
for many domains are selection and manipulation. For example, in CAD applications or 3D data
analysis, the user often wants to specify a particular point on a virtual object, or a 
destination for a new mesh to be created. Traditional methods of selecting a point on an
object generally use "picking", which involves casting a ray from the viewpoint through the 
cursor and selecting the nearest point on the object\cite{study1}. However, with a 2 DOF
interface it is extremely difficult to specify an arbitrary point in space; indeed in existing 
CAD applications users rely on multiple orthographic projections to decompose coordinates into
several 2D placements \cite{study2}.

Because selecting a point in 3D space is fundamental to many CAD operations, we elected to 
evaluate the performance of our novel interface on a placement task, based on the experiments
of \cite{study1, study2}. The placement task involves two objects situated in a 3D space; the
goal of the user is to select a target object and translate it to the position of the
destination object. Various forms of visual feedback are incorporated into this task to
make it more convenient for users; these include orthographic projections, a cursor, color
changes, and other depth cues such as shadows. Alternate evaluation tasks exclude the
translation step, only testing selection \cite{holodesk}, or examining more complex tasks
that involve a rotation as well as a translation (known as "docking") \cite{masliah2000measuring}.

\subsection{Interaction Techniques}
In our experiment we examined two aspects of 3D interaction interfaces: output interfaces and
input interfaces. Our primary interest in investigating output interfaces was whether natural depth cues,
specifically stereoscopy, were better suited to 3D tasks than artificial ones, such as the
standard orthographic projections. In terms of input interfaces, we compare the traditional 
mouse interface with a natural user interface that used gesture. 

Our goals in comparing both output and input interfaces were based on the belief that the
efficacy of input devices were closely tied to the feedback provided to the user, i.e. the
output methodology. In particular, we hypothesized that natural stereoscopic
depth cues, when spatially coupled with a gestural in-air interface, would make full use of
physical intuition. With this interface, users could conceive of the task as simply reaching
out to a physical object and moving it to another location with their hands, instead of
indirectly controlling an abstract cursor with their actions.

\subsubsection{Input Interfaces}\\
Both input interfaces involved a cursor element that was controlled by the user's actions. This
element could select and release appropriate virtual objects if the cursor overlapped the object.
However, with the coupled 3D-gesture interface, the cursor could be ignored since it was
calibrated to be at the user's fingertip.

Due to the 2DOF nature of the mouse element, cursor movements were decomposed into two planes.
By default, moving the mouse would move the cursor in the $xy$ plane parallel to the monitor.
The cursor was locked within the screen boundaries. This corresponded to the familiar mouse pointer
manipulation used in the desktop. By holding down the spacebar, the cursor would instead move in the
$xz$ plane, parallel to the table surface. This had an intuitively correspondence to mouse movements; moving
the mouse towards the screen would move the cursor into the screen. The $z$ distance was limited so that the
cursor could not move behind the viewpoint of the scene. The $xz$ motion actually affected the actual $y$-coordinate
in a way that corresponded with the perspective projection. For example, if the cursor near the bottom of the interaction
space, moving the cursor into the screen also moved it downwards in the interaction space; this meant that the screen
coordinates of the cursor would not change with this interaction. An alternate method would have kept the $y$ coordinate
constant during the $xz$ motion, so that the cursor would actually move up or down in screen coordinates as well. However,
we found during preliminary experiments that the perspective-corrected method was much more natural for 2D views, and did
not have a large impact when combined stereoscopic 3D views.

We experimented with two gestural interfaces. The first involved the thumb and index finger of a hand. The cursor would
follow the user's hand when the thumb and index finger were extended and the remaining fingers were curled. The cursor
location was between the thumb and index finger; selection and release were accomplished by pinching the two fingers
together and bringing them apart, respectively. This interface was designed such that moving an object simply felt like
picking it up and dropping it. This interface used the hand pose tracking in the Intel Perceptual Computing SDK. 
However, we found that pose recognition
was not robust under arbitrary orientations of the hand with respect to the depth camera. This interfered significantly with
the placement task, since the system would detect false releases fairly often. The closed-source nature of the SDK meant that
we could not modify the tracking algorithms for our setting, either. Thus, for the placement task interface, we elected to
relax the constraint of having a completely gestural interface, instead allowing the use of the keyboard for selection and
deselection. In this simpler interface, the depth camera takes the blob of points nearest to the screen as the cursor location.
The spacebar was the analogue of the left mouse button in the mouse interface, so that the user would hold down the spacebar
to select and move the virtual object, and release the spacebar to release the object. Users would use an extended fingertip as
their pointing tool. This method suffered from occasional problems at extremes of the interaction space (where other parts of the hand
might be closer to the screen than the fingertip), but these did not severely affect task performance. 

\subsubsection{Output Interfaces}\\
Both output interfaces incorporated similar feedback elements. Three geometric elements
appeared on the screen: a red icosahedron, representing the object to be manipulated, a
cyan sphere, representing the destination for the icosahedron, and a green tetrahedron, 
denoting the cursor. The manipulable icosahedron changed color to blue when the cursor
overlapped it to indicate that it was selectable. When it was selected and was moving
with the cursor, the icosahedron turned green. Similarly, the target sphere turned dark
cyan when the cursor was overlapping it. These feedback elements made the criterion for
performing each action in the task evident -
any movement, selection, or release condition would be clearly indicated to prevent any user
confusion. This is a fundamental user interface design principle \cite{bravenuiworld}.

The output interfaces differed in their presentation of depth cues. In the 2D output mode,
the main view of the screen was a perspective projection of the interaction space that
included an orthographic projection of a top view of the scene to indicate depth. The 3D output mode eliminated the top
view, but rendered the perspective projection in pop-out 3D so that the scene appeared to
float in front of the monitor. The spatial coordinates of the cursor in the gesture interface, when combined
with the stereoscopic view, ensured that there was an exact correspondence between the user's fingertip
and the cursor location. With the 2D display, this spatial correspondence was still present but could not
be aligned with a real-world perception of the world. The indirect correspondence of 2D and in-air interface is the one
often used by novel 3D interaction modes such as the Wiimote in-air interface used in \cite{study1}.

We also experimented with a traditional four-view 2D output
scheme, with top, side, and front orthographic projections in addition to the perspective projection.
This was motivated by the fact that most existing CAD tools use this combination of views as their
primary display modality. However, we found that our conception of the cursor as having its own
3D location conflicted with the primary interaction paradigm of this system. Normally, in CAD programs
the cursor is tied to a single view at any time; the particular plane of interaction depends on which view
the cursor is in. In contrast, conceiving of the cursor as a scene element with its own 3D location means
that the cursor is located in all of the views. This was confusing to our preliminary testers, who did not
have extensive 3D modelling experience; thus we elected to keep the intuitive perspective view as the
main display and rely on a smaller top projection reminiscent of the "minimap" projection used in many
video games to convey depth information.

% ------------------------------------- USER STUDY ----------------------------------------
% FIXME
\section{User Study}
\subsection{Procedure}
% Error, size, jitter, randomization 
\subsection{Results}
% ------------------------------------- DISCUSSION ----------------------------------------
\section{Discussion}
% FIXME
\subsection{Other Factors}
% Task type, learning curve, head tracking, jitter, experience with similar systems
% ------------------------------------- CONCLUSION ----------------------------------------
\section{Conclusion}
% FIXME

\newpage
\bstctlcite{bstctl:etal, bstctl:nodash, bstctl:simpurl}
\bibliographystyle{IEEEtranS}
\bibliography{references}

\end{document}

