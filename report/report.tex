\documentclass[pageno]{jpaper}
\newcommand{\IWreport}{2012}
\usepackage[normalem]{ulem}
\usepackage{setspace}

\begin{document}

\title{Holograsp: Bringing Virtual Objects into the Real World for Natural 3D Manipulation}

\date{}
\author{Edward Zhang}
\maketitle
\doublespacing
\thispagestyle{empty}

\begin{abstract}
Traditional computer interfaces, namely the mouse and monitor, are inherently 2D. Thus, performing 3D tasks using
these interfaces is often challenging because both input and output are missing a dimension. Surprisingly,
previous work has consistently found the mouse to be superior to custom devices with three or more degrees
of freedom (DOF). We hypothesize that the disconnect between the input device and the visual feedback is the
source of the poor performance of high DOF devices. To investigate this hypothesis, we developed the Holograsp
system. Holograsp uses a stereoscopic 3D monitor to display virtual 3D objects as if they were floating in
space in front of the monitor; users can reach out and grasp these models to move them, as if they were real
objects. We performed an experiment comparing the efficacy of Holograsp with traditional 2D input and output
interfaces by measuring the time taken to complete 3D placement tasks. The results show that, although the
pop-out stereoscopic effects were difficult for some people to perceive, the combination of stereoscopic 3D
feedback and natural gestural interface was by far the most effective and intuitive system.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
% FIXME
In this paper we make two main contributions:
\begin{itemize}
\item We present Holograsp, a 3D manipulation system, to show the feasibility of colocating display and gestural spaces.
\item We show the benefits of such systems through a user study involving 3D manipulation tasks, in which we compare the
performance of Holograsp with traditional interfaces.
\end{itemize}

% ------------------------------------- RELATED WORK ----------------------------------------
\section{Related Work}
% FIXME
\subsection{Virtual and Augmented Reality}
\subsection{3D User Interfaces}
% In air interfaces, selection/manipulation, stereoscopy, reach in systems, virtual reality, augmented reality
% ------------------------------------- OVERVIEW --------------------------------------------
\section{Holograsp Overview}
% FIXME
% ------------------------------------- IMPLEMENTATION --------------------------------------
\section{Holograsp Implementation}
\subsection{Software Architecture}
Holograsp uses a heavily abstracted engine to facilitate substitution of input and output devices, client applications, and SDKs.
The architecture of the engine invites the construction of many client applications; Holograsp provides the alignment of the
gestural and display spaces while the client program is free to use the display and input in any way. Our example application
is the timed 3D manipulation task harness used in the experiments section. The incarnation of the architecture used in the
experiments used DirectX for graphics rendering, the Nvidia API for configuring the 3D monitor, and the Intel Perceptual
Computing SDK for input.
\subsubsection{Holograsp Engine}$ $\\
The core engine uses the SDL library \footnote{http://www.libsdl.org/} for fundamental windowing and event handling. 
The engine performs the following tasks:
\begin{itemize}
\item Check for and handle mouse, keyboard, and application events.
\item Check for new data from gestural input devices.
\item Buffer and preprocess any new data into a standard form, specifically finger coordinates and depth maps.
\item Transform the raw data into application-specific data, such as parsing finger coordinates into a hand pose.
\item Call application specific update functions, such as translating and rotating objects or creating new models.
\item Render the scene in stereoscopic 3D.
\end{itemize}

Most of these tasks are delegated to their own handlers, which are often application-specific.

\subsection{Stereoscopic Display Hardware}
One of the core aspects of Holograsp is the stereoscopic 3D. There are two competing standards for stereoscopic 3D rendering for computers
that are tied to the main graphics card manufacturers, Nvidia and AMD. Nvidia's 3D Vision system \footnote{http://www.nvidia.com/object/3d-vision-main.html}
produces packaged hardware systems including high quality shutter glasses and wireless synchronization stations that are guaranteed to work with Nvidia
graphics cards and supported monitors; the AMD HD3D system does not provide a unified system \footnote{http://www.amd.com/us/products/technologies/amd-hd3d/Pages/hd3d.aspx}.

In our setup, we used an Nvidia GTX555 consumer graphics card with a 27 inch Acer VG278H 3D monitor. The standard Nvidia shutter glasses use infrared
signals to synchronize the shutters with the display; since all of the depth cameras we tested used infrared for depth information, this resulted in
unacceptable interference. To eliminate this interference, we used the shutter glasses and synchronization station from the Nvidia 3D Vision Pro
package, which uses radio frequencies for synchronization.

\subsection{Stereoscopic Rendering}
Nvidia's 3D Vision supports two methods of rendering in stereoscopic 3D. Quad-buffered stereo rendering allows developers to render the scene for each
eye separately in OpenGL, and provides maximal control of stereo effects. 3D Vision Automatic allows any application running using Direct3D to be
displayed in stereoscopic 3D, without any additional development overhead \cite{nvidia3dvision}. Quad-buffered stereo is only supported on higher-end
professional graphics cards, whereas 3D Vision Automatic is supported on all newer Nvidia graphics cards. Therefore, we elected to use 3D Vision Automatic
for stereoscopic display.

In 3D Vision Automatic, rendering in stereoscopic 3D is based on the use of homogeneous coordinates. 3D positions and transformations are represented
as 4D vectors and matrices, respectively, to allow for translation transformations \cite{graphicstextbook}. 3D Vision Automatic takes advantage of the
fact that, in homogenous coordinates, the 3D location of $(x, y, z, w)$ is equivalent to that of $(ax, ay, az, aw)$ for $a \ne 0$. The $w$-coordinate
of the coordinate vector is used in the standard projection matrix to store the eye-space depth; in normal rendering the developer generally assumes
$w=1$ and ignores the $w$ coordinate. In 3D Vision Automatic the $w$ coordinate is used to control the degree of stereoscopic effect, and the
developer can artificially set $w$ if appropriate \cite{nvidia3dvision}.

The stereoscopic effects in 3D Vision Automatic are controlled via abstracted options, primarily the eye separation and convergence depth.
3D Vision Automatic renders stereoscopically by duplicating any Direct3D calls that render to the backbuffer, and modifying the appropriate
transformation matrices so that the duplicated rendering instructions appear to have been issued for two eyes. The $x$-coordinate of any geometric
coordinates $(x_{2d}, y, z, w)$ is shifted into two stereo coordinates $(x_{left}, y, z, w)(x_{right}, y, z, w)$ based on the equations
$$x_{left} = x_{2d} + s(w - c)$$
$$x_{right} = x_{2d} - s(w - c)$$
where $s$ is the eye separation and $c$ is the convergence depth \cite{nvidia3dvision}. We note in particular that $w<c$ gives out of screen effects, where
the left eye image is to the right of the right eye image and vice versa. Unfortunately, these two quantities $s, c$ are in eye coordinates and not
physical coordinates, which necessitates a further remapping step for proper calibration.

Because of the constraints on stereoscopic rendering methods, Holograsp supports both OpenGL and Direct3D rendering APIs under a unified interface.
Currently, only simpler calls such as camera manipulation, transformation matrices, point rendering, and polygon rendering are supported. In addition,
we include fairly simple mesh rendering functionality. This general rendering interface is exposed to client applications. It is somewhat surprising that
a unified wrapper does not already exist, given the popularity of OpenGL and Direct3D; this component of Holograsp is one that can be easily extended for
more advanced rendering capabilities.

\subsection{Gestural Interface}
\subsubsection{Gestural Input Hardware and SDKs} $ $\\
Our input interface is centered around the Creative Interactive Gesture Camera and the Intel Perceptual Computing SDK 
\footnote{http://software.intel.com/en-us/vcsource/tools/perceptual-computing-sdk}.
The CIGcam includes a microphone, a standard RGB camera, and a depth camera. Holograsp made heavy use of the depth camera functionality. 
The depth camera uses infrared time-of-flight methods to construct a depth image of resolution $320 \times 240$ pixels at a peak frame
rate of 30 frames per second. The RGB camera was used to construct point-cloud visualizations but was not used for gesture recognition;
the RGB camera provides resolutions of up to $1280 \times 720$ pixels at 30 frames per second. The microphone was not used in Holograsp.

The Intel Perceptual Computing SDK provided two capabilities that were heavily used in Holograsp. The more fundamental capability was
to expose the depth and camera streams so that Holograsp could extract and process individual data frames. The SDK also provided
hand and finger pose recognition, providing the client application with precise locations of hands, palms, and fingers. 

\subsubsection{Alternate Gestural Input Devices}$ $\\
Many developments in 3D sensing hardware have been announced recently, starting with Microsoft's Kinect sensor. These sensing
devices are geared towards consumers and are inexpensive compared to earlier depth cameras, which were only used for specialized applications.
The CIGcam and its SDK were publicly released in October 2012; before acquiring a CIGcam, Holograsp used the Microsoft Kinect for Windows SDK 
\footnote{http://www.microsoft.com/en-us/kinectforwindows/} for input. In contrast to the CIGcam, the Kinect uses structured light
to obtain its depth images. The characteristics of each device are compared in Table~\ref{tab:comparison}.

\begin{table}[h]
\begin{tabular}{|c|c|}
\hline
\bf{Microsoft Kinect} & \bf{CIGcam} \\ \hline
Interaction range of 40cm to 400cm & Interaction range of 15cm to 100cm \\
$640 \times 480$ depth image resolution & $320 \times 240$ depth image resolution \\
Depth image holes at high curvature points & No holes \\
Moderate depth noise & Heavy depth noise \\
Noise typically under 3cm & Noise typically under 5cm \\
SDK focused on whole-body skeletal tracking & SDK focused on finger and face tracking \\
\$150USD in cost & \$150 USD in cost \\
\hline
\end{tabular}
\caption{Comparison of Microsoft Kinect and Creative Interactive Gesture Cam}
\label{tab:comparison}
\end{table}

Ultimately, we selected the CIGcam for its more appropriate range of interaction and the finger-recognition
capabilities of the Intel SDK.

The Leap sensor by LeapMotion \footnote{https://leapmotion.com/} is a promising sensor that we anticipate will provide extremely stable and
accurate hand tracking. Unfortunately, although we are members of the Leap early developer program, the device was not available
before user studies could take place. Future iterations of Holograsp will likely incorporate the Leap sensor and SDK.

\subsection{Gestural Input Processing}
There are two components to handling gestural input. The Layer-1 input processor extracts raw data from input devices, usually through specific
SDKs, and translates it into a standardized format for Holograsp applications. The second layer takes this data and performs heavier
parsing to extract higher level information from it. This includes translating finger coordinates into a hand pose, or extracting
points of interest from a depth image. This two layer system facilitates the substitution of alternate input devices separately from
gesture recognition algorithms.

Input device SDKs usually consists of a depth image at minimum. A depth image is a grid of pixels, where each 
pixel contains the distance from the camera to the nearest object along the ray through that pixel. Many SDKs
also provide a color image from a camera, as well as positions of hands, fingers, heads, or other body parts.
Layer-1 input processing classes tended to be simple wrappers for calls to the appropriate SDK, using a unified
interface.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/fsm.png}
\caption{Finite State Machine for Hand Selection States}
\label{fig:fsm}
\end{figure}
Holograsp currently has two Layer-2 input processing classes. The first examines finger coordinates to distinguish poses based
on the index finger and the thumb; the point of interest (used for a cursor or a selection point) would be the midpoint of the tip
of the thumb and the tip of the index finger, and different states were triggered based on whether the two fingers were pinched
together or were separate. This was accomplished via a simple finite state machine illustrated in Figure~\ref{fig:fsm}. We also
applied hysteresis to the state transitions to account for tracking noise; 5 consecutive frames indicating the same state transition
were necessary before a state transition occured. 

The second Layer-2 class has a simpler and tends to be more robust. The mechanism simply selects the closest blob of pixels to the monitor
as the point of interest. For speed and simplicity, the default algorithm looks at neighborhoods of nine pixels to determine the closest blob of pixels,
and takes the average of the corresponding points in space to be the point of interest. This mechanism also applies some simple filtering to 
smooth each coordinate of the resulting point, since this input method was subject to varying amounts of jitter. Several simple 
smoothing methods can be used; we found a double moving average filter with a window size of 5 to be appropriate for the data rates of
our input devices. Many filters are included in Holograsp's implementation, mostly adapted from \cite{brown2004smoothing}. This class was
used in the experiment application because of its robustness.

\subsection{Calibration of Coordinate Systems}
As Holograsp involves a spatially accurate input and output systems, there are two distinct calibration steps needed. The calibration
of camera coordinates with world coordinates is a well-understood problem in computer vision and augmented reality. 
However, it is much harder to calibrate the stereoscopic display space given the limitations of the 3D Vision Automatic
system, and more approximate methods must be used.
\subsubsection{Camera to World Calibration}$ $\\
To convert camera coordinates into world coordinates, we used the simple fiducial marker tracker included in the ARToolkit software
library \footnote{http://www.hitl.washington.edu/artoolkit/}, first introduced in \cite{artoolkit}. This determines the camera's position
and orientation in the form of a matrix $M_{RT}$ mapping homogeneous coordinates in world space to camera space.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/hiro.png}
\caption{Fiducial Marker used in ARToolkit}
\label{fig:hiro}
\end{figure}

Camera calibration using fiducial markers is very well understood and is a fundamental part of most augmented reality applications. Fiducial
markers, such as the one depicted in Figure~\ref{fig:hiro}, are generally composed of a thick black border and an inner area 
containing distinguishing features. The standard algorithm first thresholds the camera frame and looks for regions that are composed of four straight lines.
This yields the equations of two perpendicular line segments, as projected onto the camera plane. Using the perspective projection equation 
for the camera, which depends only on the camera's field of view and aspect ratio, the algorithm can compute the plane containing two
parallel line segments. This plane determines the camera's orientation with respect to the marker. The translation of the camera can then be
easily obtained by substituting the coordinates of the fiducial marker corners into the projection equation. This algorithm is described in
detail in \cite{kato1999marker}.
\subsubsection{Stereoscopic Display to World Calibration}$ $\\
The major benefit of using a stereoscopic 3D monitor instead of a head-mounted display is that the 3D calibration is less sensitive to
head movements when using the monitor. We operated under the assumption that the user would be approximately 80cm away from the monitor
and the neutral eye position would be directed at the middle of the monitor. We also provide users with a mechanism to fine tune the
calibration, since we could not guarantee identical head positioning each time the system was used.

We formulated the display coordinate to world coordinate calibration as a linear transformation.

$$M_{d} = \left(
\begin{array}{cccc}
a & 0 & a_z & a_t \\
0 & b & b_z & b_t \\
0 & 0 & c_z & c_t \\ 
0 & 0 &   0 &   1
\end{array}\right)$$

This matrix is considerably simpler than a full rotation-translation matrix. This simplification was based on several assumptions:
\begin{itemize}
\item True stereoscopic rendering, using quad-buffered stereo, requires the location and orientation of the viewer and the eye separation.
Camera intrinsic, perspective, and transformation matrices are all linear, as is the stereoscopic approximation used in Nvidia 3D Vision
Automatic. in particular, the $c$ depth convergence parameter is simply an artificial depth offset. We therefore felt justified in choosing
a linear transformation to represent the display to world calibration matrix.
\item Nvidia 3D Vision Automatic does not allow for head roll; this is a fairly safe operating assumption since people generally do 
not tilt their heads to the side significantly when working with visual displays. Thus, we assume that the $x$ and $y$ coordinates are
independent. This makes the process of obtaining measurements much simpler, since we can indepently calibrate the $x$ and $y$ dimensions.
\item We also assume that objects at a constant distance from the monitor will have the same $z$ coordinate, regardless
of position on the monitor. This relies on the assumption that the user is approximately centered with respect to the monitor. This is
a safe assumption because it is fairly straightforward for the user to tell if they are severely off-center, and small deviations
do not affect the calibration significantly. Therefore, the $z$ coordinate is independent of the $x$ and $y$ coordinates.
\item $a_z, b_z$ are necessary because the display space is actually a frustrum; the base of the frustrum is located at the corners of
the monitor, and the apex is at the user's head. Therefore the distance from the monitor affects the world $x,y$ coordinates at a given
pixel; for example, for an object near the top of the monitor, moving further away from the display makes the apparent world coordinates of the
object closer to the center of the display.
\end{itemize}

Initial estimates for these values were obtained by displaying models with very clear depth cues at the center of the screen at ten screen $z$ depths,
with screen coordinates $(0,0,z)$. The apparent location of the model in world coordinates, $(x_w, y_w, z_w)$, was measured by hand. This gave the following equations
$$a_zz + a_t = x_w$$
$$b_zz + b_t = y_w$$
$$c_zz + c_t = z_w$$
We then performed a linear regression on each coordinate to obtain $a_z, a_t, b_z, b_t, c_z, c_t$. 
Then we display the model at ten $z$ depths at the corners of the monitor, with coordinates $(w/2, h/2, z)$ where $w,h$ are the horizontal and vertical
resolution of the monitor, respectively.
$$\frac{aw}{2} + a_zz + a_t = x_w$$
$$\frac{bh}{2} + b_zz + b_t = y_w$$
Linear regressions on the $x$ and $y$ coordinates independently give values for $a,b$ as well as validate our assumption of a linear dependence of $x$ on $z$.

Our fine-tuning calibration was mainly used to adjust the translation parameters $(a_t, b_t, c_t)$. All users during our user study required at least
a small amount of adjustment of these parameters; this was done by eye. We also provided mechanisms to adjust
$(a_z, b_z, c_z)$, since these would change depending on the distance from the monitor. However, we found during our user study that the small range of
head distances and the moderate size of the interaction space meant that adjusting $(a_t, b _t, c_t)$ was sufficient to calibrate points in the
interaction space.

In future iterations of Holograsp, we aim to include an automated calibration workflow. This would involve displaying models at several locations and
having the user indicate gesturally where the objects appeared. We would also like to include at least rudimentary head tracking, involving fiducial
markers or more sophisticated computer vision techniques to recognize heads in depth images.

% ------------------------------------- METHDOLOGY ------------------------------------------
\section{Evaluation Methodology}
Many previous works, such as \cite{study1,mattheiss2011navigating,study2}, have consistently found the
mouse to be faster and more precise for 3D manipulation
tasks than devices with higher degrees of freedom, including in-air devices. 
The goal of this experiment was to determine if the poor performance of high DOF devices could
be a result of cognitive separation between the interface and the task to be performed.
\subsection{3D Manipulation Task}
Bowman et. al describe several fundamental tasks in 3D interfaces, primarily selection,
manipulation, navigation, system control, and symbolic input \cite{3dui}. The most fundamental
for many domains are selection and manipulation. For example, in CAD applications or 3D data
analysis, the user often wants to specify a particular point on a virtual object, or a 
destination for a new mesh to be created. Traditional methods of selecting a point on an
object generally use "picking", which involves casting a ray from the viewpoint through the 
cursor and selecting the nearest point on the object\cite{study1}. However, with a 2 DOF
interface it is extremely difficult to specify an arbitrary point in space; indeed in existing 
CAD applications users rely on multiple orthographic projections to decompose coordinates into
several 2D placements \cite{study2}.

Because selecting a point in 3D space is fundamental to many CAD operations, we elected to 
evaluate the performance of our novel interface on a placement task, based on the experiments
of \cite{study1, study2}. The placement task involves two objects situated in a 3D space; the
goal of the user is to select a target object and translate it to the position of the
destination object. Various forms of visual feedback are incorporated into this task to
make it more convenient for users; these include orthographic projections, a cursor, color
changes, and other depth cues such as shadows. Alternate evaluation tasks exclude the
translation step, only testing selection \cite{holodesk}, or examining more complex tasks
that involve a rotation as well as a translation (known as "docking") \cite{masliah2000measuring}.

\subsection{Interaction Techniques}
In our experiment we examined two aspects of 3D interaction interfaces: output interfaces and
input interfaces. Our primary interest in investigating output interfaces was whether natural depth cues,
specifically stereoscopy, were better suited to 3D tasks than artificial ones, such as the
standard orthographic projections. In terms of input interfaces, we compare the traditional 
mouse interface with a natural user interface that used gesture. 

Our goals in comparing both output and input interfaces were based on the belief that the
efficacy of input devices were closely tied to the feedback provided to the user, i.e. the
output methodology. In particular, we hypothesized that natural stereoscopic
depth cues, when spatially coupled with a gestural in-air interface as in Holograsp, would make full use of
physical intuition. With this interface, users could conceive of the task as simply reaching
out to a physical object and moving it to another location with their hands, instead of
indirectly controlling an abstract cursor with their actions.

\subsubsection{Input Interfaces}$ $\\
Both input interfaces involved a cursor element that was controlled by the user's actions. This
element could select and release appropriate virtual objects if the cursor overlapped the object.
However, with the coupled 3D-gesture interface, the cursor could be ignored since it was
calibrated to be at the user's fingertip.

Due to the 2DOF nature of the mouse element, cursor movements were decomposed into two planes.
By default, moving the mouse would move the cursor in the $xy$ plane parallel to the monitor.
The cursor was locked within the screen boundaries. This corresponded to the familiar mouse pointer
manipulation used in the desktop. By holding down the spacebar, the cursor would instead move in the
$xz$ plane, parallel to the table surface. This had an intuitively correspondence to mouse movements; moving
the mouse towards the screen would move the cursor into the screen. The $z$ distance was limited so that the
cursor could not move behind the viewpoint of the scene. The $xz$ motion actually affected the actual $y$-coordinate
in a way that corresponded with the perspective projection. For example, if the cursor near the bottom of the interaction
space, moving the cursor into the screen also moved it downwards in the interaction space; this meant that the screen
coordinates of the cursor would not change with this interaction. An alternate method would have kept the $y$ coordinate
constant during the $xz$ motion, so that the cursor would actually move up or down in screen coordinates as well. However,
we found during preliminary experiments that the perspective-corrected method was much more natural for 2D views, and did
not have a large impact when combined stereoscopic 3D views.

We experimented with two gestural interfaces. The first involved the thumb and index finger of a hand. The cursor would
follow the user's hand when the thumb and index finger were extended and the remaining fingers were curled. The cursor
location was between the thumb and index finger; selection and release were accomplished by pinching the two fingers
together and bringing them apart, respectively. This interface was designed such that moving an object simply felt like
picking it up and dropping it. This interface used the hand pose tracking in the Intel Perceptual Computing SDK. 
However, we found that pose recognition
was not robust under arbitrary orientations of the hand with respect to the depth camera. This interfered significantly with
the placement task, since the system would detect false releases fairly often. The closed-source nature of the SDK meant that
we could not modify the tracking algorithms for our setting, either. Thus, for the placement task interface, we elected to
relax the constraint of having a completely gestural interface, instead allowing the use of the keyboard for selection and
deselection. In this simpler interface, the depth camera takes the blob of points nearest to the screen as the cursor location.
The spacebar was the analogue of the left mouse button in the mouse interface, so that the user would hold down the spacebar
to select and move the virtual object, and release the spacebar to release the object. Users would use an extended fingertip as
their pointing tool. This method suffered from occasional problems at extremes of the interaction space (where other parts of the hand
might be closer to the screen than the fingertip), but these did not severely affect task performance. 

\subsubsection{Output Interfaces}$ $\\
Both output interfaces incorporated similar feedback elements. Three geometric elements
appeared on the screen: a red icosahedron, representing the object to be manipulated, a
cyan sphere, representing the destination for the icosahedron, and a green tetrahedron, 
denoting the cursor. The manipulable icosahedron changed color to blue when the cursor
overlapped it to indicate that it was selectable. When it was selected and was moving
with the cursor, the icosahedron turned green. Similarly, the target sphere turned dark
cyan when the cursor was overlapping it. These feedback elements made the criterion for
performing each action in the task evident -
any movement, selection, or release condition would be clearly indicated to prevent any user
confusion. This is a fundamental user interface design principle \cite{bravenuiworld}.

The output interfaces differed in their presentation of depth cues. In the 2D output mode,
the main view of the screen was a perspective projection of the interaction space that
included an orthographic projection of a top view of the scene to indicate depth. The 3D output mode eliminated the top
view, but rendered the perspective projection in pop-out 3D so that the scene appeared to
float in front of the monitor. The spatial coordinates of the cursor in the full Holograsp interface, when combined
with the stereoscopic view, ensured that there was an exact correspondence between the user's fingertip
and the cursor location. With the 2D display, this spatial correspondence was still present but could not
be aligned with a real-world perception of the world. The indirect correspondence of 2D and in-air interface is the one
often used by novel 3D interaction modes such as the Wiimote in-air interface used in \cite{study1}.

We also experimented with a traditional four-view 2D output
scheme, with top, side, and front orthographic projections in addition to the perspective projection.
This was motivated by the fact that most existing CAD tools use this combination of views as their
primary display modality. However, we found that our conception of the cursor as having its own
3D location conflicted with the primary interaction paradigm of this system. Normally, in CAD programs
the cursor is tied to a single view at any time; the particular plane of interaction depends on which view
the cursor is in. In contrast, conceiving of the cursor as a scene element with its own 3D location means
that the cursor is located in all of the views. This was confusing to our preliminary testers, who did not
have extensive 3D modelling experience; thus we elected to keep the intuitive perspective view as the
main display and rely on a smaller top projection reminiscent of the "minimap" projection used in many
video games to convey depth information.

% ------------------------------------- USER STUDY ----------------------------------------
\section{User Study}
\subsection{Procedure}
The user study was centered around the 3D placement task. Volunteers were solicited by 
through word of mouth and departmental Facebook groups. We collected data from 14 participants,
consisting of 13 males and one female. One male participant was a graduate student in computer
science; 10 participants were undergraduates in computer science and the remaining participants
concentrated in East Asian Studies, Mathematics, and Astrophysics.

Before the task participants were asked to fill out a survey describing their experience with
gestural systems, stereoscopic 3D, CAD and 3D modelling, and video gaming. 

The participants were then introduced to the system to familiarize themselves with the placement task, the pop-out
stereoscopy and the gesture system. An experimenter was with the participant at all times during
the use of the system. The participants were first exposed to the pop-out stereoscopic effects;
if they had trouble perceiving the objects they were given an explanation of how the stereoscopy
worked and some simple eye exercises to try. Then they were reintroduced to the stereoscopic effects
at a slightly less intense level where objects appeared closer to the screen, and they were encouraged
to interact with the system using the mouse interface. All participants were then introduced to the placement
task using the 3D-mouse interface and encouraged to finish five to ten tasks to demonstrate their understanding.
The participant was then introduced to the full Holograsp 3D-gesture interface, including the intuition behind the aligned gesture and display
spaces and the calibration procedure. They once again had the opportunity to finish up to ten tasks using this interface.

After familiarizing themselves with the interface, the participants then performed the full round of timing experiments. These
experiments used a within-factor design, with independent variables of output modality (2D vs. 3D) and input modality (gesture vs. mouse).
For each of the four interface combinations (2D-Mouse, 2D-Gesture, 3D-Mouse, and 3D-Gesture), the participant completed a 
run of ten consecutive placement tasks. Participants performed two blocks of these 40 tasks, for a total of $(14)(2)(40) = 1120$
placement tasks. Each run of tasks was preceded by one untimed task to give the user time to adapt to the interface, and 
in the 3D-Gesture case, perform calibration.

In each task, the locations of the target and destination objects were selected at random in the interaction space. The
ordering of the interfaces within a run was also randomized; the one exception was that the last interface in a block
would not be selected as the first interface in the subsequent block. 

The quantitative data collected was the time in milliseconds for every selection and release event. We did not formally
collect data on movement trajectories, but the experimenter did collect qualitative observations on general trends in
trajectories. After completion of the second block, participants were asked to fill out a survey involving several questions
about their perceptions of the interfaces. We asked participants to compare input modalities and output modalities, as well as
make general observations about the efficacy of the system. We also asked a series of short questions asking participants for
the interface choices best fitting a certain criterion, as well as a short justification. These criterion were:
\begin{itemize}
\item Their personal favorite
\item The easiest to use for completing the task
\item The most intuitive
\item The most accurate
\item The one they'd choose if they had to perform the task many times
\item The one they'd recommend to beginners
\end{itemize}
% Error, size, jitter, randomization, trajectories
\subsection{Results}
Figure~\ref{fig:aggregatedistr} shows the distribution of times and mean times per task for each interface, considering time until first selection, time from first selection until
task completion, and total task completion time. These plots show that the gestural interfaces had more variation in completion times, especially in high completion-time
outliers. Furthermore, an ANOVA on the results showed a strong effect of interface on completion time ($F_{3,840}=27.546, p < 0.0001$). 
However, no general conclusions can be drawn between the mouse interfaces or the 3D gestural interface; post-hoc analysis showed no significant differences
in mouse interface times ($p > 0.01$) or between the mouse and either mouse interface ($p > 0.01$)

Because of the lack of conclusive results, we analyzed post-hoc the results for each individual participant. This data revealed several more interesting observations.
The most significant observation was that most people were either significantly faster with both mouse interfaces, or significantly faster with the 3D-Gesture interface.
This contrasts with the aggregated data, which would suggest that each participant would have similar completion times between the mouse interfaces and the 3D-Gesture
interface. Inspection of survey results revealed that the critical distinction whether participants had expressed difficulty with perceiving
the pop-out stereoscopic effects. Figure~\ref{fig:sepmean} compares the mean time to task completion for each interface and each stereoscopic perception group.

Because of the large differences in task performance across individuals, we also examined the performance of individuals relative to their own completion times.


%TODO: Make Figures, t-test or ANOVA, graphs for distribution and mean
% ------------------------------------- DISCUSSION ----------------------------------------
\section{Discussion}
% FIXME
\subsection{Other Factors}
% Task type, learning curve, head tracking, jitter, experience with similar systems
% ------------------------------------- CONCLUSION ----------------------------------------
\section{Conclusion}
% FIXME

\newpage
\bstctlcite{bstctl:etal, bstctl:nodash, bstctl:simpurl}
\bibliographystyle{IEEEtranS}
\bibliography{references}

\end{document}

