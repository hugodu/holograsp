\documentclass[pageno]{jpaper}
\newcommand{\IWreport}{2012}
\usepackage[normalem]{ulem}
\usepackage{setspace}

\begin{document}

\title{Holograsp: Bringing Virtual Objects into the Real World for Natural 3D Manipulation}

\date{}
\author{Edward Zhang}
\maketitle
%\doublespacing
\onehalfspace
\thispagestyle{empty}

\begin{abstract}
Traditional computer interfaces, namely the mouse and monitor, are inherently 2D. Thus, performing 3D tasks using
these interfaces is often challenging and unintuitive. Surprisingly,
previous work has consistently found the mouse to be superior for 3D tasks to custom devices with three or more degrees
of freedom (DOF). We postulate that there is a disconnect between input devices and visual feedback which is the
source of the poor performance of high DOF devices. To investigate this hypothesis, we developed the Holograsp
system. Holograsp uses a stereoscopic 3D monitor to display virtual 3D objects as if they were floating in
space in front of the monitor; users can reach out and grasp these models to move them, as if they were real
objects. By colocating the display positions of virtual objects with the interaction space of a gestural input system,
Holograsp eliminates the gap between input device and visual feedback.
We performed an experiment comparing the efficacy of Holograsp with traditional 2D input and output
interfaces by measuring the average time taken to complete 3D placement tasks. Although 
pop-out stereoscopic effects were difficult for some participants to perceive, Holograsp was the fastest
and most intuitive interface for the remaining users. This suggests that the alignment of stereoscopic 3D
feedback and a natural gestural interface is a very effective design for 3D user interfaces.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
We live in a world of three spatial dimensions, which gives us an intuitive grasp on how to interact with
physical objects. If we see a book on a bookshelf, we can reach out and grab the correct book with
absolute confidence in where our hand is going. However, this physical intuition is absent in current 3D interactive
systems. In modern 3D modelling and CAD tools, we are forced to view 3D objects on 2D monitors, and have to
select, move, and rotate objects and our viewpoint using a keyboard and 2 DOF mouse. This often results in unintuitive and
tedious workflows; to move an object in a 3D virtual world, we have to unnaturally decompose the movement of the
object into several planar components, usually through the use of orthogonal projections (such as the top view, side view, and front view),
until the placement is correct.
What if we could achieve this placement task as fluidly and naturally as if we were manipulating a real object? This
physical intuitiveness is the motivation behind the development of the Holograsp system presented in this paper.

Several existing systems have already attempted to make use of physical intuition by developing high DOF input 
devices to interact with 3D information. These input methods range from desktop joystick-like
devices \cite{mattheiss2011navigating}, to handheld proxies for virtual objects \cite{mine1997moving}, to empty-handed gestural interactions realized via 
external sensors \cite{manders2010gesture}.
However, numerous studies have found that these devices are inferior to the mouse in terms of speed and accuracy \cite{mattheiss2011navigating, study1}.
We posit that the major problem with these systems is their focus on only half of the factors relevant for intuition: although they focus on the intuitiveness of 
the input device, the visual feedback is rarely adapted to react seamlessly with the input device. Instead, the natural input device is used to manipulate an onscreen object, far
removed from being able to intuitively reach out to the object. In our bookshelf example, we must manipulate the book at the end of a long stick, instead of
instinctively grabbing it. Our hypothesis is that, if we colocate the gestural interaction space with a 3D display space with appropriate depth cues, we can regain
the full intuitiveness of interacting with physical objects. Imagine that your bookshelf, physically located on the other side of the room, was virtually displayed as if it were right
in front of you; you can once again reach out and "grab" your desired book intuitively, as if it was real.

In this paper, we make two main contributions:
\begin{itemize}
\item We present Holograsp, a 3D manipulation system, to show the feasibility of colocating display and gestural spaces.
\item We show the benefits of this coupled gestural input and visual output through a user study involving 3D manipulation tasks, in which we compare the
performance of Holograsp with traditional interfaces.
\end{itemize}

To motivate these contributions, we first inspect related studies and systems in 3D user interfaces and mixed reality. We then show how Holograsp fits
into the existing work by describing the design of Holograsp, including the software architecture, input and output device considerations,
and the alignment process. We then present our experimental methodology to justify our evaluation methods. Finally, we discuss the results
of our experiments and provide some possibilities for future research. 

% ------------------------------------- RELATED WORK ----------------------------------------
\section{Background and Related Work}
\subsection{Mixed Reality}
Effective usage of computer systems depends on how users conceptualize their interactions with the system.
Perhaps the most obvious method is to present computer data in a physically motivated setting, adapting
human intuition about the physical world into natural interactions with the system. Because of the nature of 3D data,
it is especially appropriate to conceptualize 3D interactions in physical terms by analogy with the real world. Milgram and Kishino present
the virtuality continuum of {\bf Mixed Reality} environments, describing the degree of overlap between the
real world environment and the system's virtual environment, in \cite{milgram1994taxonomy}. Real environments, namely
the real world, contain only real objects, and {\bf Virtual Reality} (VR) environments, such as 3D worlds in video games, contain only
virtual objects. A well-known intermediate between these two extremes is {\bf Augmented Reality} (AR), in which virtual
objects are placed within the real world. such as the CAD modelling systems in the Iron Man films. Another intermediate is
{\bf Augmented Virtuality}, in which some real objects (usually the user's own body) are placed within a virtual world; a common
example of augmented virtuality is green-screening, where a user interacts in front of a blank screen but is displayed in a
different, virtual world.

\subsection{Virtual Reality}
There has been a significant amount of academic work published on virtual reality, especially
in {\bf immersive virtual reality}, where the user perceives themselves to be in the virtual environment. This is in
contrast with virtual worlds rendered on monitors, which provide a window into the virtual world instead of complete immersion.
Immersive
virtual reality systems generally use Head-Mounted Displays (HMDs) for complete visual immersion, and have been studied
since 1968, when Sutherland introduced the first HMD \cite{sutherland1968head}. Although immersive virtual reality
systems provide perceptual realism, interaction with such systems is difficult because users cannot physically see any input devices.
Some systems use the human body as the main interface controller, such as \cite{laviola1999whole, laviola1999flex}. Other systems render
virtual analogues of physical input devices, such as the virtual tablet in \cite{poupyrev1998virtual} or the cubes used in \cite{mine1997exploiting}.
Another common interaction strategy is to use multimodal input, especially the combination of physical devices and voice recognition systems, as in \cite{bolt1980put}.
In all of these systems, visual feedback is critical for effective interaction in the virtual world.

\subsection{Augmented Reality}
In contrast to virtual reality, {\bf Augmented Reality} involves placing virtual objects within the real world. This is a challenging task that involves two
components: determining the location of the virtual object relative to the display, and rendering the object within the real environment. \cite{zhou2008trends}.

To render virtual objects, an AR system needs to know where the display is in relation to real world objects; this usually involves tracking the position and orientation a camera attached to the
display. Currently, the dominant tracking strategy in AR is to use fiducial markers. Fiducial markers are two-dimensional shapes placed in the environment
that can be used to determine the relative position and orientation of a camera using computer vision techniques. Works using fiducial markers include
\cite{kato1999marker, artoolkit, dedual2011creating}. Newer methods based on fiducial tracking can use arbitrary 2D images or structures as markers \cite{ferrari2001markerless, simon2000markerless}.
However, neither fiducial markers nor these marker-based techniques function
well if the markers are blurred, partially occluded, out of view, inconsistently lighted, or at extreme angles; some of these challenges are investigated in \cite{fiala2010designing}. Other methods of tracking include inertial
tracking, which use accelerometers and gyroscopes without an external frame of reference \cite{lang2002inertial}, and markerless tracking, which uses optical flow and
stereo reconstruction techniques to determine 3D motion of a camera \cite{comport2006real}.

Early AR systems focused on head mounted displays, often using {\bf video see-through} techniques, where a small display in front of the user's eyes showed the augmented video
stream from cameras located directly in front of the displays. This was effective because, like in immersive VR systems, the user perceived themselves as being inside the augmented environment.
However, many drawbacks such as high latency, low field of view, and imprecise tracking \cite{hoffman2008vergence} have resulted in a shift towards different display methods.
Modern commercial and academic augmented reality systems now tend to use smartphones and tablets as "windows" into the augmented world. These devices are well suited for such applications because
of the integrated camera and display, their availability and portability, and their array of high-quality sensors. In this usage, virtual objects are overlaid on top of the
device's camera image on the screen, so that moving the "window" around presents different views of the real environment; such systems
include DRIVE \cite{kim2011drive} and T(ether) \footnote{http://kiwi.media.mit.edu/tether/}.
\subsection{Colocation of Hands and Virtual Manipulation Controllers}
Mixed reality systems often focus on the human hand for natural interaction. 
Although several 3D manipulation systems use direct mappings between hand position and a virtual hand, such as \cite{poupyrev1996go},
comparatively few aim to make the perceived 3D position of the hand proxy be coincident with the user's natural perception of their
hand location. The relevant perceptual phenomenon, known as {\bf proprioception}, is a person's automatic knowledge of the position and orientation of their
limbs relative to themselves. The use of proprioception in 3D interfaces was investigated by Mine in \cite{mine1997exploiting}, which found that
in an immersive virtual environment, manipulating objects colocated with the users hand was more efficient than manipulation at an offset. Several systems,
such as \cite{mulder2002personal, prachyabrued2011dropping}, use a two-layered tabletop with a mirrored display above a physical table; in these systems the interaction space, underneath
the display and above the table, is directly mapped to what the user sees in the display. The aforementioned systems are strictly virtual reality systems, since the actual environments displayed
are still completely synthesized. Recent work by Microsoft Research, such as the Holodesk \cite{holodesk}, has instead used half-silvered mirrors to create an augmented reality
interaction space, where the user can see their hands interacting in the same world as virtual objects. Holodesk focused on realistic physical interactions within the space,
representing real objects as point clouds that can knock over or hold up other objects. As far as we are aware, Holodesk is the only existing system to augment a real
interactive space with colocated virtual objects.

\subsection{Evaluation of 3D User Interfaces}
Several studies have investigated the effectiveness of novel, high-DOF input devices for 3D tasks by directly comparing them to traditional devices, especially the mouse.
Evaluation methods generally involve performing fundamental 3D tasks, especially selection, placement, and rotation. Input methods are compared based on their
speed and accuracy. The SpaceNavigator, a commercially available high DOF device, was found to be inferior to the mouse for placement tasks in
\cite{mattheiss2011navigating}. B{\'e}rard et al also found the mouse to be more effective than several high-DOF devices for placement tasks
in \cite{study1}.  However, a later study by the same group found that enhanced visual feedback, in the form of pop-up depth views, greatly enhanced the effectiveness
of the high-DOF device, and was found to perform better than the mouse \cite{study2}. The
evaluation of the Holodesk system also involved a user study, in which they compared performance on a selection task with varying depth cues. This study focused on
the effectiveness of aligning the location of physical objects in the tabletop space with the apparent locations of virtual objects. They found that aligned
stereoscopic display systems were better than aligned non-stereoscopic systems with appropriate occlusion cues, and unaligned displays were slowest \cite{holodesk}.
% ------------------------------------- IMPLEMENTATION --------------------------------------
\newpage
\section{The Holograsp System}
\subsection{Overview}
Holograsp is a software system that provides the capability of gesture recognition aligned with pop-out stereoscopic 3D rendering. 
It does this by converting coordinates of a 3D input device into real-world coordinates, and then converting the 
apparent positions of the stereoscopically rendered objects into the same real-world coordinates. 
Holograsp provides several different methods of stereoscopic rendering, as well as
a generic interface for depth input devices. These components allow a variety of 3D interactive applications to be built on top of Holograsp;
for the purposes of this paper we implemented an application that measured performance on a 3D translation task.
\subsection{Software Architecture}
Holograsp uses a heavily abstracted engine to facilitate substitution of input and output devices, and invites the construction of
client applications; Holograsp provides the alignment of the
gestural and display spaces while the client program is free to use the display and input in any way desired. Our example client application
is the timed 3D manipulation task harness used in the experiments section. The incarnation of the architecture used in the
experiments used DirectX for graphics rendering, the Nvidia API for configuring the 3D monitor, and the Intel Perceptual
Computing SDK for input processing.
The core application engine uses the SDL library \footnote{http://www.libsdl.org/} for fundamental windowing and event handling.
The engine performs the following tasks:
\begin{itemize}
\item Check for and handle mouse, keyboard, and application events.
\item Check for new data from gestural input devices.
\item Buffer and preprocess any new data into a standard form, specifically positions of fingers and depth data.
\item Transform this raw data into application-specific data, such as parsing finger coordinates into a hand pose.
\item Call application specific update functions, such as translating and rotating virtual objects, creating new models, or navigating menus.
\item Render the scene in stereoscopic 3D.
\end{itemize}

Most of these tasks are delegated to their own handlers, which are often application-specific.

\subsection{Stereoscopic Display Hardware}
One of the core components of Holograsp is the stereoscopic 3D display. Stereoscopic displays involve providing each eye with a different image, providing the
depth cues of stereopsis and convergence. There are many different technologies used for stereoscopy, but the dominant ones for consumers use polarizing glasses
(passive 3D) such as those used in movie theaters, shutter glasses that alternately block each lens (active 3D), or lenticular lenses (glasses-free 3D) that use optics 
to direct different images in different directions. Computer monitors generally use active 3D systems, which involves a 120Hz refresh rate synchronized with shutter glasses.
Alternate frames display the image for one eye while the other lens of the shutter glasses is darkened.
In addition to stereoscopic hardware, there are two competing standards for consumer stereoscopic 3D rendering 
developed by the dominant graphics card manufacturers, Nvidia and AMD. Nvidia's 3D Vision system \footnote{http://www.nvidia.com/object/3d-vision-main.html}
produces packaged hardware systems including high quality shutter glasses and wireless synchronization stations that are guaranteed to work with Nvidia
graphics cards and supported monitors; the AMD HD3D system does not provide such a unified system \footnote{http://www.amd.com/us/products/technologies/amd-hd3d/Pages/hd3d.aspx}.

In our setup, we used an Nvidia GTX555 consumer graphics card and a 27 inch Acer VG278H 3D monitor with a resolution of $1920\times 1024$, giving a 16:9 aspect ratio. 
The standard Nvidia shutter glasses use infrared
signals to synchronize the shutters with the display; since all of the depth cameras we tested used infrared for depth information, this resulted in
unacceptable interference. To eliminate this interference, we used the shutter glasses and synchronization station from the Nvidia 3D Vision Pro
package, which uses radio frequencies for synchronization.

\subsection{Stereoscopic Rendering}
Nvidia's 3D Vision supports two methods of rendering in stereoscopic 3D. Quad-buffered stereo rendering exposes buffers that allow developers to render the scene for each
eye separately in OpenGL; this provides maximal control of stereo effects since the developer can control exactly what goes into each eye's image.
In contrast, 3D Vision Automatic alters an application running using DirectX's Direct3D so that it is
displayed in stereoscopic 3D, without any additional development overhead \cite{nvidia3dvision}. Quad-buffered stereo is only supported on higher-end
professional graphics cards, whereas 3D Vision Automatic is supported on all newer Nvidia graphics cards. Therefore, we elected to use 3D Vision Automatic
for stereoscopic display.

In 3D Vision Automatic, rendering in stereoscopic 3D is based on the use of homogeneous coordinates. 3D positions and transformations are represented
as 4D vectors and matrices, respectively, to allow for translation transformations \cite{graphicstextbook}. 3D Vision Automatic takes advantage of the
fact that, in homogenous coordinates, the 3D location of $(x, y, z, w)$ is equivalent to that of $(ax, ay, az, aw)$ for $a \ne 0$. The $w$-coordinate
of the coordinates is used in the standard projection matrix to store the eye-space depth; normally the developer assumes
$w=1$ and ignores the $w$ coordinate. In 3D Vision Automatic the $w$ coordinate is used to control the degree of stereoscopic effect, and the
developer can artificially set $w$ if appropriate \cite{nvidia3dvision}.

The stereoscopic effects in 3D Vision Automatic are controlled by generic, non-application-specific settings; the user can manually control
eye separation and convergence depth settings.
3D Vision Automatic renders stereoscopically by duplicating any Direct3D calls that render to the screen, and modifying the appropriate
transformation matrices so that the duplicated rendering instructions appear to have been issued twice, once for each eye. The $x$-coordinate of any geometric
coordinates $(x_{2d}, y, z, w)$ is shifted into two stereo coordinates $(x_{left}, y, z, w)(x_{right}, y, z, w)$ based on the equations
$$x_{left} = x_{2d} - s(w - c)$$
$$x_{right} = x_{2d} + s(w - c)$$
where $s$ is the eye separation and $c$ is the convergence depth \cite{nvidia3dvision}. We note in particular that $w<c$ gives out of screen effects, where
the left eye image is to the right of the right eye image and vice versa. Unfortunately, these two quantities $s, c$ are in eye coordinates and not
physical coordinates, which necessitates a complicated remapping step for proper calibration to real-world coordinates.

Because of the multiple stereoscopic rendering methods provided by graphics manufacturers, Holograsp supports both OpenGL and Direct3D rendering APIs under a unified interface.
Currently, only fundamental calls such as camera manipulation, transformation matrices, point rendering, and polygon rendering are supported. In addition,
we include fairly simple mesh rendering functionality. This general rendering interface is exposed to client applications. It is somewhat surprising that
a unified wrapper for OpenGL and Direct3D does not already exist, given the popularity of both APIs. This component of Holograsp is one that can be easily extended for
more advanced rendering capabilities and used in other applications.

\subsection{Gestural Input Hardware and SDKs}
Our input interface is centered around the Creative Interactive Gesture Camera (CIGcam) and the Intel Perceptual Computing SDK
\footnote{http://software.intel.com/en-us/vcsource/tools/perceptual-computing-sdk}.
The CIGcam includes a microphone, a standard RGB camera, and a depth camera. Holograsp made heavy use of the depth camera functionality.
The depth camera uses infrared time-of-flight methods to construct a depth image of resolution $320 \times 240$ pixels at a peak frame
rate of 30 frames per second. The RGB camera was used to construct point-cloud visualizations but was not used for gesture recognition;
the RGB camera provides resolutions of up to $1280 \times 720$ pixels at 30 frames per second. The microphone was not used in Holograsp.

The Intel Perceptual Computing SDK provided two capabilities that were important to Holograsp. It exposed the basic depth and 
camera streams so that Holograsp could extract and process individual data frames. The SDK also provided
hand and finger pose recognition, providing us with precise locations of hands, palms, and fingers.

Many developments in 3D sensing hardware have been announced recently, starting with Microsoft's Kinect sensor. These sensing
devices are geared towards consumers and are inexpensive compared to earlier depth cameras, which were only used for specialized applications.
The CIGcam and its SDK were publicly released in October 2012; before acquiring a CIGcam, Holograsp used the Microsoft Kinect for Windows SDK
\footnote{http://www.microsoft.com/en-us/kinectforwindows/} for input. In contrast to the CIGcam, the Kinect uses structured light
to obtain its depth images. The characteristics of each device are compared in Table~\ref{tab:comparison}.
Ultimately, we selected the CIGcam for its more appropriate range of interaction and the finger-recognition
capabilities of the Intel SDK.


\begin{table}[h]
\begin{tabular}{|c|c|}
\hline
{\bf Microsoft Kinect} & {\bf CIGcam} \\ \hline
Interaction range of 40cm to 400cm & Interaction range of 15cm to 100cm \\
$640 \times 480$ depth image resolution & $320 \times 240$ depth image resolution \\
Depth image holes at high curvature regions & No holes \\
Moderate depth noise & Heavy depth noise \\
Noise typically under 3cm & Noise typically under 5cm \\
SDK focused on whole-body skeletal tracking & SDK focused on finger and face tracking \\
\$150 USD in cost & \$150 USD in cost \\
\hline
\end{tabular}
\caption{Comparison of Microsoft Kinect and Creative Interactive Gesture Cam}
\label{tab:comparison}
\end{table}

The Leap sensor by LeapMotion \footnote{https://leapmotion.com/} is a promising sensor that we anticipate will provide extremely stable and
accurate hand tracking. Unfortunately, although we are members of the Leap early developer program, the device was not available
before user studies could take place. Future iterations of Holograsp will likely incorporate the Leap sensor and SDK.

\subsection{Gestural Input Processing}
There are two software levels that handle gestural input. The Layer-1 input processor extracts raw data from input devices, usually through specific
SDKs, and translates it into a standardized format for Holograsp applications. The second layer takes this data and performs heavier
parsing to extract higher level information from it. This includes translating finger coordinates into a hand pose, or extracting
points of interest from a depth image. This two layer system facilitates the substitution of alternate input devices separately from
gesture recognition algorithms.

Input device SDKs usually consists of a depth image. A depth image is a grid of pixels, where each
pixel contains the distance from the camera to the nearest object along the ray through that pixel. Many SDKs
also provide a color image from a camera, as well as positions of hands, fingers, heads, or other body parts.
Layer-1 input processing classes tend to be simple wrappers for calls to the appropriate SDK, using a unified
interface.

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{figures/fsm.png}
\caption{Finite State Machine for Hand Selection States\\
The Intel SDK provides the coordinates of hands and fingers, as well as confidence levels in these coordinates. Our FSM
examines the presence or absence of a hand, as well as the number of fingers detected, to determine transitions between
hand states. When the thumb and forefinger are extended from a hand, the SDK detects two fingers. 
When the thumb and forefinger are pinched together, they are detected as one finger; the tip of this finger is the point
of interest for selection}
\label{fig:fsm}
\end{figure}

Holograsp currently has two Layer-2 input processing classes. The first examines finger coordinates to distinguish poses based
on the index finger and the thumb; the point of interest (used for a cursor or a selection point) is located halfway between the tip
of the thumb and the tip of the index finger, and different states are triggered based on whether the two fingers were pinched
together or were separate. This was accomplished via a simple finite state machine illustrated in Figure~\ref{fig:fsm}. We also
applied hysteresis to the state transitions to account for tracking noise; five consecutive frames indicating the same state transition
were necessary before a state transition occured.

The second Layer-2 class is simpler and tends to be more robust, but does not provide multiple tracking states. The mechanism simply selects the closest blob of pixels to the monitor
as the point of interest. For speed and simplicity, the algorithm looks at neighborhoods of nine pixels to determine the closest blob,
and takes the average position of the corresponding points in space to be the point of interest. This mechanism also applies some simple filtering to
smooth the coordinates of the resulting point, since this input method was subject to jitter. Several simple
smoothing methods can be used; we found a double moving average filter with a window size of 5 to be appropriate for the data rates of
our input devices. Many filters alternate are included in Holograsp's implementation, mostly adapted from \cite{brown2004smoothing}. This Layer-2 class was
used in the experiment application because of its robustness.

\subsection{Calibration of Coordinate Systems}
As Holograsp involves a spatially accurate input and output systems, there are two distinct calibration steps needed. The calibration
of camera coordinates with world coordinates is a well-understood problem in computer vision and augmented reality.
However, it is much harder to calibrate the stereoscopic display space given the limitations of the 3D Vision Automatic
system, so approximate methods must be used.
\subsubsection{Camera to World Calibration}$ $\\
To convert camera coordinates into world coordinates, we used the simple fiducial marker tracker included in the ARToolkit software
library \footnote{http://www.hitl.washington.edu/artoolkit/}, first introduced in \cite{artoolkit}. This determines the camera's position
and orientation in the form of a rotation-translation matrix mapping homogeneous coordinates in world space to camera space.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figures/hiro.png}
\caption{Fiducial Marker used in ARToolkit}
\label{fig:hiro}
\end{figure}

Camera calibration using fiducial markers is very well understood and is a fundamental part of most augmented reality applications. Fiducial
markers, such as the one depicted in Figure~\ref{fig:hiro}, are generally composed of a thick black border and an inner area
containing distinguishing features. The standard algorithm first thresholds a camera frame by brightness and looks for regions that are bounded by four straight lines.
This is under the assumption that the fiducial markers will be the clearest such regions.
This yields the equations of two sets of parallel line segments, as projected onto the camera plane. Using the perspective projection equation
for the camera, which depends only on the camera's field of view and aspect ratio, the algorithm can compute the plane containing each pair of
parallel line segments. This plane determines the camera's orientation with respect to the marker. The camera translation can then be
obtained by substituting the coordinates of the fiducial marker corners into the projection equation. Once this transformation is determined, the image within
the black border can be normalized and analyzed to distinguish it from other active fiducial markers in the scene. This algorithm is described in
detail in \cite{kato1999marker}.
\subsubsection{Stereoscopic Display to World Calibration}$ $\\
The major benefit of using a stereoscopic 3D monitor instead of a head-mounted display is that the 3D calibration is less sensitive to
head movements when using the monitor. We operated under the assumption that the user would be approximately 80cm away from the monitor
and the neutral eye position would be directed at the middle of the monitor. We also provide users with a mechanism to fine tune the
calibration, since we could not guarantee identical head positioning each time the system was used.

Display coordinates are oriented such that the positive $z$ axis projects out of the screen towards the user, the positive $y$ axis points up, and the positive $x$ axis points to the user's right.
For simplicity, we orient the real-world coordinate system in the same way. This allows us to formulate the display coordinate to world coordinate calibration as a linear transformation.

$$M_{d} = \left(
\begin{array}{cccc}
a & 0 & a_z & a_t \\
0 & b & b_z & b_t \\
0 & 0 & c_z & c_t \\
0 & 0 &   0 &   1
\end{array}\right)$$

This matrix is considerably simpler than a full rotation-translation matrix. This simplification was based on several assumptions:
\begin{itemize}
\item True stereoscopic rendering, using quad-buffered stereo, depends on the location and orientation of the viewer, as well as the distance between
their eyes.
Camera intrinsic, perspective, and transformation matrices are all linear, as is the stereoscopic approximation used in Nvidia 3D Vision
Automatic. in particular, the $c$ depth convergence parameter is simply an artificial depth offset. We therefore felt justified in choosing
a linear transformation to represent the display to world calibration matrix.
\item Nvidia 3D Vision Automatic does not allow for head roll; this is a fairly safe operating assumption since people generally do
not tilt their heads to the side significantly when working with visual displays. Thus, we assume that the $x$ and $y$ coordinates are
independent. This makes calibration much simpler, since we can indepently calibrate the $x$ and $y$ dimensions.
\item We also assume that objects at a constant distance from the monitor will have the same $z$ coordinate, regardless
of the $xy$ position on the monitor. This relies on the assumption that the user is approximately centered with respect to the monitor. This is
a safe assumption because it is fairly straightforward for the user to tell if they are severely off-center, and small deviations
do not affect the calibration significantly. Therefore, the $z$ coordinate is independent of the $x$ and $y$ coordinates.
\item $a_z, b_z$ are necessary because the display space is actually a frustrum; the base of the frustrum is located at the corners of
the monitor, and the apex is at the user's head. Therefore the distance from the monitor affects the world $x,y$ coordinates at a given
pixel; for example, for an object near the top of the monitor, moving further away from the display makes the apparent world coordinates of the
object closer to the center of the display.
\end{itemize}

Initial estimates for these values were obtained by displaying models with very clear depth cues at the center of the screen at ten screen $z$ depths,
with screen coordinates $(0,0,z)$. The apparent location of the model in world coordinates, $(x_w, y_w, z_w)$, was measured by hand. This gave the following equations
$$a_zz + a_t = x_w$$
$$b_zz + b_t = y_w$$
$$c_zz + c_t = z_w$$
We then performed a linear regression on each coordinate to obtain $a_z, a_t, b_z, b_t, c_z, c_t$.
Then we display the model at ten $z$ depths at the corners of the monitor, with known display coordinates $(x, y, z)$, and again measure the world coordinates
by hand.
$$ax + a_zz + a_t = x_w$$
$$by + b_zz + b_t = y_w$$
$$c_zz + c_t = z_w$$
Linear regressions on the $x$ and $y$ coordinates independently give values for $a,b$ as well as validate our assumption of a linear dependence of $x,y$ on $z$.

Our fine-tuning calibration was mainly used to adjust the translation parameters $(a_t, b_t, c_t)$. All users during our user study required at least
a small amount of adjustment of these parameters; this was done by eye. We also provided mechanisms to adjust
$(a_z, b_z, c_z)$, since these would change depending on the distance from the monitor. However, we found during our user study that the small range of
head distances and the moderate size of the interaction space meant that adjusting $(a_t, b _t, c_t)$ was sufficient to calibrate points in the
interaction space.

In future iterations of Holograsp, we aim to include an automated calibration workflow. This would involve displaying models at several locations and
having the user indicate gesturally where the objects appeared. We would also like to include at least rudimentary head tracking, involving fiducial
markers or more sophisticated computer vision techniques to recognize heads in depth images.

% ------------------------------------- METHDOLOGY ------------------------------------------
\newpage
\section{Evaluation Methodology}
Many previous works, such as \cite{study1,mattheiss2011navigating,study2}, have consistently found the
mouse to be faster and more precise for 3D manipulation
tasks than devices with higher degrees of freedom, including in-air devices.
The goal of this experiment was to determine if the poor performance of high DOF devices could
be remedied by aligning the visual feedback with the input device to reduce cognitive separation between the interface and the task to be performed
\subsection{3D Manipulation Task}
Bowman et. al describe several fundamental tasks in 3D interfaces, namely selection,
manipulation, navigation, system control, and symbolic input \cite{3dui}. The most fundamental
for many domains are selection and manipulation. For example, in CAD applications or 3D data
analysis, the user often wants to specify a particular point on a virtual object, or a
destination for a new object to be created. Traditional methods of selecting a point on an
object generally use "picking", which involves casting a ray from the viewpoint through the
cursor and selecting the nearest point on the object\cite{study1}. However, with a 2 DOF
interface it is extremely difficult to specify an arbitrary point in space; indeed in existing
CAD applications users rely on multiple orthographic projections to decompose translations into
several 2D placements \cite{study2}.

Because selecting a point in 3D space is fundamental to many CAD operations, we elected to
evaluate the performance of our novel interface on a placement task, based on the experiments
of \cite{study1, study2}. The placement task involves two objects situated in a 3D space; the
goal of the user is to select a target object and translate it to the position of the
destination object. Various forms of visual feedback are incorporated into this task to
make it more convenient for users; these include orthographic projections, a cursor, color
changes, and other depth cues such as shadows. Alternative tasks could exclude the
translation step, only testing selection \cite{holodesk}, or include more complex tasks
that involve a rotation as well as a translation (known as "docking") \cite{masliah2000measuring}.

\subsection{Interaction Techniques}
In our experiment we examined the two components of 3D interaction interfaces: output interfaces and
input interfaces. Our primary interest in investigating output interfaces was whether natural depth cues,
specifically stereoscopy, were better suited to 3D tasks than artificial ones, such as the
standard orthographic projections. For input interfaces, we compare the traditional
mouse interface with a natural user interface that used gesture.

Our goals in comparing both output and input interfaces were based on the belief that the
efficacy of input devices was closely tied to the feedback provided to the user, in other words, the
output methodology. In particular, we hypothesized that natural stereoscopic
depth cues, when spatially coupled with a gestural in-air interface as in Holograsp, would make full use of
physical intuition. With this interface, users could imagine the task as simply reaching
out to a physical object and moving it to another location with their hands, instead of
indirectly controlling an abstract cursor with their actions.

\subsubsection{Input Interfaces}$ $\\
Both input interfaces involved a cursor element that was controlled by the user's actions. The user
could select and release virtual objects if the cursor overlapped the object.
However, with the coupled 3D-gesture interface, the cursor could be ignored since it was
calibrated to be at the user's fingertip.

Due to the 2DOF nature of the mouse interface, cursor movements were decomposed into two planes.
By default, moving the mouse would move the cursor in the $xy$ plane parallel to the monitor.
The cursor was locked within the screen boundaries. This corresponded to the familiar mouse pointer
manipulation paradigm used in the desktop. By holding down the spacebar, the cursor would instead move in the
$xz$ plane, parallel to the table surface. This had an intuitive correspondence to mouse movements; moving
the mouse towards the screen would move the cursor into the screen. The $z$ distance was limited so that the
cursor could not move behind the scene viewpoint (i.e. behind the user's eyes). The $xz$ motion actually affected the actual $y$-coordinate
in a way that corresponded with the perspective projection. For example, if the cursor was near the bottom of the interaction
space, moving the cursor into the screen also moved it upwards in the interaction space; as a result the screen
coordinates of the cursor would not change when the mouse was moved perpendicular to the screen. Alternatively, we could have kept the $y$ coordinate
constant during any $xz$ motion, so that the cursor would appear move up or down in screen coordinates as well. However,
we found during preliminary experiments that the perspective-corrected method was much more natural for 2D views, and did
not have a large impact when combined with stereoscopic 3D views.

We experimented with two gestural interfaces. The first involved the thumb and index finger of a hand. The cursor would
follow the user's hand when the thumb and index finger were extended and the remaining fingers were curled. The cursor
location was between the thumb and index finger; selection and release were accomplished by pinching the two fingers
together and bringing them apart, respectively. This interface was designed such that moving an object simply felt like
picking it up and dropping it. This interface used the hand pose tracking in the Intel Perceptual Computing SDK.
However, we found that pose recognition
was not robust under arbitrary orientations of the hand with respect to the depth camera. This interfered significantly with
the placement task, since the system would detect false releases of the object fairly often. The closed-source nature of the SDK meant that
we could not modify the tracking algorithms for our setting, either. Thus, for the placement task interface, we elected to
relax the constraint of having a completely gestural interface, instead allowing the use of the keyboard for selection and
deselection. In this simpler interface, the depth camera takes the blob of points nearest to the screen as the cursor location.
The spacebar was the analogue of the left mouse button in the mouse interface, so that the user would hold down the spacebar
to select and move the virtual object, and release the spacebar to release the object. Users would use an extended fingertip as
their pointing tool. This method suffered from occasional problems at extremes of the interaction space (where other parts of the hand
might be closer to the screen than the fingertip), but these did not severely affect task performance.

\subsubsection{Output Interfaces}$ $\\
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{figures/labelledexperiment.png}
\caption{3D Manipulation Task Display \\
The elements displayed in this figure correspond to the 2D multiview output mode. The stereoscopic
output mode does not have the top view in the upper left corner, but the remaining objects are displayed
such that they appear in front of the plane of the monitor.}
\label{fig:experimentdisplay}
\end{figure}
Both output interfaces incorporated similar feedback elements. These elements are depicted in
Figure~\ref{fig:experimentdisplay}. Three geometric elements
appeared on the screen: a red icosahedron, representing the object to be manipulated, a
cyan sphere, representing the destination for the icosahedron, and a green tetrahedron,
denoting the cursor. The manipulable icosahedron changed color to blue when the cursor
overlapped it to indicate that it was selectable. When it was selected and was moving
with the cursor, the icosahedron turned green. Similarly, the target sphere turned dark
cyan when the cursor was overlapping it. These feedback elements made the criterion for
performing each action in the task evident -
any movement, selection, or release condition would be clearly indicated to prevent any user
confusion. This is a fundamental user interface design principle \cite{bravenuiworld}.

The output interfaces differed in their presentation of depth cues. In the 2D output mode,
the main view of the screen was a perspective projection of the interaction space that
included an orthographic projection of a top view of the scene to indicate $z$ depth. The 3D output mode eliminated the top
view, but rendered the perspective projection in pop-out 3D so that the scene appeared to
float in front of the monitor. The spatial coordinates of the cursor in the full Holograsp interface, when combined
with the stereoscopic view, ensured that there was an exact correspondence between the user's fingertip
and the cursor location. With the 2D display and gesture interface, this spatial correspondence was still present but could not
be aligned with a real-world perception of the world. The indirect correspondence of 2D and in-air interface is the one
often used by novel 3D interaction modes such as the Wiimote in-air interface used in \cite{study1}.

We also experimented with a traditional four-view 2D output
scheme, with top, side, and front orthographic projections in addition to the perspective projection.
This was motivated by the fact that most existing CAD tools use this combination of views as their
primary display modality. However, we found that our conception of the cursor as having its own
3D location conflicted with the primary interaction paradigm of this system. Normally, in CAD programs
the cursor is tied to a single view at any time; the particular plane of interaction depends on which view
the cursor is in. In contrast, conceiving of the cursor as a scene element with its own 3D location means
that the cursor is located in all of the views. This was confusing to our preliminary testers; thus we elected to keep the intuitive perspective view as the
main display and rely on a smaller top projection reminiscent of the "minimap" projection used in many
video games to convey depth information.

% ------------------------------------- USER STUDY ----------------------------------------
\newpage
\section{User Study}
\subsection{Procedure}
The user study was centered around the 3D placement task. Volunteers were solicited
through word of mouth and departmental Facebook groups. We collected data from 14 participants,
consisting of 13 males and one female. One male participant was a graduate student in computer
science; 10 participants were undergraduates in computer science and the remaining participants
concentrated in East Asian Studies, Mathematics, and Astrophysics.

Before the task participants were asked to fill out a survey describing their experience with
gestural systems, stereoscopic 3D, CAD and 3D modelling, and electronic gaming.
The participants were then introduced to the system to familiarize themselves with the placement task, the pop-out
stereoscopy and the gesture system. An experimenter was with the participant at all times during
the use of the system. The participants were first exposed to the pop-out stereoscopic effects;
if they had trouble perceiving the objects they were given an explanation of how the stereoscopy
worked and some simple eye exercises to try. Then they were reintroduced to the stereoscopic effects
at a more moderate level where objects did not pop out as much, and they were encouraged
to interact with the system using the mouse interface. All participants were then introduced to the placement
task using the 3D-mouse interface and instructed to finish five to ten tasks to demonstrate their understanding.
The participant was then introduced to the full Holograsp 3D-gesture interface, including the intuition behind the aligned gesture and display
spaces and the calibration procedure. They once again had the opportunity to finish up to ten tasks using this interface.

After familiarizing themselves with the interface, the participants then performed the full round of timing experiments. These
experiments used a within-subjects design, with independent variables of output modality (2D vs. 3D) and input modality (gesture vs. mouse).
For each of the four interface combinations (2D-Mouse, 2D-Gesture, 3D-Mouse, and 3D-Gesture), the participant completed a
run of ten consecutive placement tasks. Participants performed two blocks of these 40 tasks, for a total of $(14)(2)(40) = 1120$
placement tasks. Each run of tasks was preceded by one untimed task to give the user time to adapt to the interface, and
in the 3D-Gesture case, perform calibration.

In each task, the locations of the target and destination objects were selected uniformly at random in the interaction space. The
ordering of the interfaces within a run was also randomized; the one exception was that the last interface in a block
would not be selected as the first interface in the subsequent block.

The quantitative data collected was the time in milliseconds for every selection and release event. We did not formally
collect data on movement trajectories, but we did make qualitative observations on general trends in
trajectories. After completion of the second block, participants were asked to fill out a survey involving several questions
about their perceptions of the interfaces. We asked participants to compare input modalities and output modalities, as well as
make general observations about the efficacy of the system. We also asked a series of short questions asking participants for
the interface choices best fitting a certain criterion, including a short justification. These criteria were:
\begin{itemize}
\item Their personal favorite
\item The easiest to use for completing the task
\item The most intuitive
\item The most accurate
\item The one they'd choose if they had to perform the task many times
\item The one they'd recommend to beginners
\end{itemize}
% Error, size, jitter, randomization, trajectories
\subsection{Results}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/aggmean.png}
\caption{Mean placement times for each interface in a. time until first
selection, b. time from first selection until task completion, and c. total task
completion time. All data includes 95\% confidence intervals.}
\label{fig:aggregate}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/distribution.png}
\caption{Distribution of total placement times for each interface. Whiskers are
 at most 1.5 IQR in length. Note that three outliers with completion time $> 40000$ for the 2D gesture interface are not
shown}
\label{fig:distr}
\end{figure}
Figure~\ref{fig:aggregate} shows the mean times per task for each interface, considering time until first selection, time from first selection until
task completion, and total task completion time. We also show the distribution
of mean placement times in Figure~\ref{fig:distr}. These plots show that the gestural interfaces had more variation in completion times, especially in high completion-time
outliers. Furthermore, a repeated measures ANOVA on the results showed a strong effect of
interface on completion time ($F_{3,39}=7.19, p < 0.001$).
However, no general conclusions can be drawn between the mouse interfaces or the 3D gestural interface; post-hoc analysis showed no significant differences
in mouse interface times ($p > 0.05$) or between the 3D-gesture and either mouse interface ($p > 0.05$). It is clear, however, that the 2D-gesture interface
was slower than the other interfaces ($p < 0.01$).

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/mean.png}
\caption{Mean placement times for each interface, separated by whether
stereoscopic perception problems were expressed. This data is presented with 95\% confidence intervals.}
\label{fig:sepmean}
\end{figure}
Because of the lack of conclusive results, we further analyzed post-hoc the results for each individual participant. This data revealed several more interesting observations.
Most people were either significantly faster with both mouse interfaces, or significantly faster with the 3D-Gesture interface.
This contrasts with the aggregated data, which would suggest that each participant would have similar completion times between the mouse interfaces and the 3D-gesture
interface. Inspection of survey results revealed that the critical distinction
was whether participants had expressed difficulty with perceiving
the pop-out stereoscopic effects. Figure~\ref{fig:sepmean} compares the mean
time to task completion for each interface and each stereoscopic perception
group. Pairwise t-tests show significant differences between performance between
the groups for the 3D gesture interface ($p < 0.01$) but not for any other
interfaces ($p > 0.05$).

Because of the large differences in task performance between individuals, we
also examined participant performance relative to their own completion times.
Figure~\ref{fig:mousevs3dg} shows a scatter plot with one point for each
participant. The horizontal component of each point is the participant's mean 3D
mouse time divided by their mean 3D gesture time; likewise the vertical
component of each plot is the participant's mean 2D mouse time divided by their
mean 3D gesture time. A similar plot is shown in Figure~\ref{fig:gesturevs2dm},
which compares mean completion times of the two gestural interfaces normalized
by the 2D mouse mean completion time.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/f1.pdf}
\caption{Relative completion times for 3D mouse and 2D mouse interfaces,
normalized by 3D gesture completion times.}
\label{fig:mousevs3dg}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/f2.pdf}
\caption{Relative completion times for 3D gesture and 2D gesture interfaces,
normalized by 2D mouse completion times.}
\label{fig:gesturevs2dm}
\end{figure}

From Figure~\ref{fig:mousevs3dg}, we can observe that for most participants
the two mouse interfaces were fairly similar in performance, since most points
are fairly close to the $y = x$ line in blue. The plot also shows that the
participants with poor stereoscopic perception tended to be faster with the
mouse interfaces than the gesture interface, since their data points tend to be below
and to the left of the $y = 1, x = 1$ lines. Similarly, participants without
stereoscopic perception issues are above and to the right of $y = 1, x = 1$.

From Figure~\ref{fig:gesturevs2dm}, we can see that almost all participants were
faster with the 3D gestural interface than the 2D gestural interface (most
points are above $y = x$). Likewise, almost all participants are better with the
2D mouse interface than the 2D gesture interface (most points are above $y=1$).
Finally, we note once again that the participants with good stereoscopic
perception ability tend to have faster 3D gesture times than 2D mouse times (to
the left of $x=1$) and those with poor stereoscopic perception were faster with
the 2D mouse than the 2D gesture.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/survey.png}
\caption{Interface preferences for post-experiment survey}
\label{fig:survey}
\end{figure}

We show the results of the survey for the interface preference questions in
Figure~\ref{fig:survey}. We noted that the most positively regarded interfaces were the 3D gesture and 2D mouse interfaces, while the
2D gesture interface was only preferred by one participant for a single category. The 3D gesture interface was popular as a ``favorite'' and the ``most intuitive''
interface, and was regarded as ``beginner-friendly''. Surprisingly, even participants who performed very poorly with the 3D gesture interface expressed these opinions.
Some of these participants commented that they were occasionally ``able to lock on to depth cues,'' and therefore had experienced the benefits of the aligned gesture
and display volumes. However, since it was clear to participants that the 3D gesture interface was the novel one, there was probably an unavoidable bias in the answers favoring it.
The mouse interfaces were almost unanimously voted as most accurate, which was unsurprising given the jitter of the gestural interfaces and the
2DOF stability of the mouse. The preferences for ``easiest'' and ``best [interface] for repetion'' were fairly evenly split between the mouse interfaces and the Holograsp
interface; for these categories, participants who performed well with the 3D gesture interface were more likely to record a preference for the 3D gesture interface.

% ------------------------------------- DISCUSSION ----------------------------------------
\newpage
\section{Discussion}
Overall, no conclusive evidence was found to suggest Holograsp was significantly better than existing interfaces. The difference in mean
completion times were not significant between the Holograsp interface and mouse interfaces (whether stereoscopic or 2D). However, some very
interesting results appear when we take stereoscopic perception into account. When we examine participants who expressed no difficulty with
stereoscopic perception, the Holograsp interface was far and away the best interface. In fact the four fastest mean completion times were
performed with Holograsp. This supports our initial theory that aligned gesture and display space was beneficial to performance, since weak
stereoscopic perception implies the absence of the 3D display volume central to Holograsp. However, this analysis is not conclusive, especially since these observations
were made post-hoc. It is possible, for instance, that participants wished to justify poor performance by drawing attention to stereoscopic issues.
We also note that those who did not express issues with stereoscopy on average performed better than their counterparts for all interfaces, suggesting
that spatial reasoning skills may be correlated with stereoscopic perception ability.

In future studies, the most important goal would be to formally
control for stereoscopic perception, since this seemed to play a critical role in performance. The best way to do this
would be to provide an objective test of stereoscopic ability before the timed task evaluation. We also
would like to investigate the effects of learning as well as fatigue by conducting longer experiments with
more trial blocks per person. One performance factor that we did not examine was accuracy. This is a commonly analyzed measure; the study in \cite{study1}
measures both speed and accuracy. However, analyzing speed and accuracy independently is difficult because
there is a natural tradeoff between the two. It is hard to specify to participants a desired level of accuracy without impacting speed.
One method commonly used in HCI is Fitts' Law, which gives an explicit relationship between
speed and accuracy. However, Fitts' Law was formulated for one-dimensional tasks and no generally accepted extension
to higher dimensions has been found (but see \cite{mackenzie1992extending, monk1985fitts, murata2001extending} for some analyses of Fitts-like equations for higher
dimensional interaction tasks). 

We also
considered whether the visual feedback and task might be biased for or against a specific interface combination. For example, the
complexity of the scene would affect the required accuracy for a placement task, since having more objects clustered close together would require
higher precision during selection; this might result in worse gestural performance because of jitter. Even such minor decisions as the cursor size and minimum overlap
for selection may have been more or less favorable for either input interface. Similarly the multiview 2D display interface
encourages the conceptualization of the task as two separate placements, and therefore the mouse interface might be more natural for this output.
We found the results of \cite{study2} especially intriguing,
since the addition of one new visual feedback element was enough to completely change task performance. 
In our study, it is interesting to note that no significant difference was found between the two output techniques when used with the mouse. 
Informal comments made by some participants suggested
that they relied on the color-changing visual feedback indicating when objects were overlapping for $z$ positioning, only using stereoscopy or orthographic
views to tell them whether they were in front of or behind the target.
This observation and the results of \cite{study2} strongly suggest that the details
of visual feedback are as important, if not more important, than the actual input device; it also means that application interface elements must be
designed very carefully since generic interface patterns may not be sufficient for arbitrary tasks. In general, user interaction with 3D applications are very different from the idealized tasks
of repeated placement. We would like 3D design experts to use Holograsp for actual CAD workflows, and we believe that comparing the productivity of Holograsp with
traditional interfaces in real applications would be much more illustrative than our abstract study.
Even within simplified task performance evaluation, we are interested in testing different tasks, such as rotation or docking, to determine how the nature of the
3D task affects interface choice.

One very illuminating observation was the trajectories that participants took to move the target object.
For mouse interfaces, it was fairly universal to position the shape on one plane, and then move it in the remaining one dimension 
to its final destination. For example, in the perspective view, participants would overlap the image of the icosahedron and the sphere ($xy$
alignment) and then adjust the $z$ coordinate according to depth cues. This was naturally encouraged by the decomposition of the mouse interface into two separate
modes. Surprisingly, many participants, especially those with weaker stereoscopic
perception, adopted the same decomposition when using gestural interfaces as well. In contrast, those with strong stereoscopic perception always took a direct path
that required very little fine adjustment of position. This was perhaps the major difference between the trajectories of the two stereoscopic perception ability groups. 
Quantitative analyses of movement trajectories are rare but not absent from the literature (see \cite{integralityseparability}), however
several works such as \cite{kruijff2006unconventional} visualize and qualitatively analyze trajectories. More formal quantitative trajectory analysis 
would be a fruitful area of investigation for future 3D interfaces.

It was evident from survey comments that technical details in the Holograsp interface also require significant
polishing. Ten of the participants explicitly mentioned jitter as a problem with the gestural interface. We anticipate
that using better algorithms and more accurate sensors, such as the Leap, will greatly enhance the usability of the
gestural interface. Surprisingly, no users commented on stereoscopic inconsistencies caused by occlusions. We expected
occlusion to be a significant downside to using a 3D monitor: if a virtual object appeared to be 40cm in front of the monitor, if the user
places their finger ``behind'' the object 30cm in front of the monitor, there is no way to display the object appropriately 
even though the object should appear to block the finger. We believe that this problem was unnoticed because of the sparseness of the 3D
scene and the simple manipulations involved in the study. We did have two users comment that calibration
was noticeably affected when they moved their heads. This suggests that head and viewpoint tracking would greatly enhance the 
realism of the virtual scene. The combination of head tracking and stereoscopic monitors is a common 3D display technique known as {\bf fishtank VR}
and is used in \cite{holodesk}. Although many natural depth cues are absent in the current implementation of Holograsp, it is promising that
stereoscopic cues already have a noticeable benefit for intuitive 3D interaction.

% ------------------------------------- CONCLUSION ----------------------------------------
\section{Conclusion}
In this paper, we presented the Holograsp system. Holograsp provides a natural way for users to interact with 3D data by colocating gestural interaction
space with a stereoscopic display volume. Holograsp knows that, if you place your finger on a virtual object, then you are interacting with that object at
that point. With this sytem, we allow natural interactions such as grabbing a virtual object as if it were a real one.

We also evaluated the Holograsp system by comparing it to traditional input and output systems for 3D tasks. We performed a user study to determine whether
using Holograsp was faster than other interfaces in a 3D placement task. We separated this analysis into two axes: Holograsp's 3D stereoscopic
versus traditional multiview 2D displays with orthographic projections, and the gestural in-air interface versus traditional mouse inputs. We
found that, for users who could perceive the colocated stereoscopic display, Holograsp was significantly faster than any other interface combination;
however, for many users who did experience difficulties with stereoscopic perception, mouse interfaces were noticeably faster than gestural interfaces.
This investigation showed promising results as a pilot study, but more experimentation is needed to draw definitve conclusions about these interfaces.

Despite the shortcomings of the current system, the possibilities presented by Holograsp are quite exciting. Holograsp provides a framework that
can support a full 3D modelling application with intuitive manipulation input mappings. The computer systems depicted in the Iron Man films are
an especially compelling example of where this could lead, showing possibilities for natural interaction with 3D CAD in the design of machinery
and suits, and 3D data analysis in the manipulation of maps and graphs. We aim to allow the user to treat the virtual world as part of the real
world, and Holograsp shows great promise in fulfilling this vision.

\newpage
\bstctlcite{bstctl:etal, bstctl:nodash, bstctl:simpurl}
\bibliographystyle{IEEEtranS}
\bibliography{references}

\end{document}
